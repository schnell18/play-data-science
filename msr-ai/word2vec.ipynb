{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "11aad8de-189c-45d8-a439-13e82f76b54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('Train.csv')\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['review'], data['sentiment'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d73502e9-cfe9-4cba-b9df-1e1de1c0b863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the text data\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f26edc95-9f8d-44b5-96ea-2d9e692b4139",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'apply'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(tokens)\n\u001b[0;32m----> 8\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m(preprocess)\n\u001b[1;32m      9\u001b[0m X_test \u001b[38;5;241m=\u001b[39m X_test\u001b[38;5;241m.\u001b[39mapply(preprocess)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'apply'"
     ]
    }
   ],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = ' '.join([word for word in text if word not in string.punctuation])\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "X_train = X_train.apply(preprocess)\n",
    "X_test = X_test.apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "48f1b0ca-8cc1-48fb-aa3e-a2b3f38c8df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Word2Vec model\n",
    "sentences = [sentence.split() for sentence in X_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3966348-0ff1-4006-97e9-ab21f8ba3190",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(sentences, window=5, min_count=5, workers=4)\n",
    "\n",
    "# Vectorize the text data\n",
    "def vectorize(sentence):\n",
    "    words = sentence.split()\n",
    "    words_vecs = [w2v_model.wv[word] for word in words if word in w2v_model.wv]\n",
    "    if len(words_vecs) == 0:\n",
    "        return np.zeros(100)\n",
    "    words_vecs = np.array(words_vecs)\n",
    "    return words_vecs.mean(axis=0)\n",
    "\n",
    "X_train = np.array([vectorize(sentence) for sentence in X_train])\n",
    "X_test = np.array([vectorize(sentence) for sentence in X_test])\n",
    "\n",
    "# Train a classification model\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92bc6467-8aca-4b20-a604-c77b69d3d6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.853\n",
      "Precision: 0.8478578383641675\n",
      "Recall: 0.8634110064452156\n",
      "F1 score: 0.8555637435519529\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "y_pred = clf.predict(X_test)\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('Precision:', precision_score(y_test, y_pred, pos_label=1))\n",
    "print('Recall:', recall_score(y_test, y_pred, pos_label=1))\n",
    "print('F1 score:', f1_score(y_test, y_pred, pos_label=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504020bd-1628-4d64-8df3-04c7545638e4",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03634798-4a30-432e-bd00-a2cd64bea59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "68cfb48f-7a2e-4276-83cc-409acd4dcd95",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m X_test \u001b[38;5;241m=\u001b[39m pad_sequences(X_test, maxlen\u001b[38;5;241m=\u001b[39mmax_length, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Train the Word2Vec model\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m sentences \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43msentence\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msentence\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     42\u001b[0m w2v_model \u001b[38;5;241m=\u001b[39m Word2Vec(sentences, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, window\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, min_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Create a weight matrix for the embedding layer\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[40], line 41\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     38\u001b[0m X_test \u001b[38;5;241m=\u001b[39m pad_sequences(X_test, maxlen\u001b[38;5;241m=\u001b[39mmax_length, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Train the Word2Vec model\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [\u001b[43msentence\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m() \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m X_train]\n\u001b[1;32m     42\u001b[0m w2v_model \u001b[38;5;241m=\u001b[39m Word2Vec(sentences, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, window\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, min_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Create a weight matrix for the embedding layer\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Preprocess the text data\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = ''.join([word for word in text if word not in string.punctuation])\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('Train.csv')\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['review'], data['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = X_train.apply(preprocess)\n",
    "X_test = X_test.apply(preprocess)\n",
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "# Pad the sequences to a fixed length\n",
    "max_length = 100\n",
    "X_train = pad_sequences(X_train, maxlen=max_length, padding='post')\n",
    "X_test = pad_sequences(X_test, maxlen=max_length, padding='post')\n",
    "\n",
    "# Train the Word2Vec model\n",
    "sentences = [sentence.split() for sentence in X_train]\n",
    "w2v_model = Word2Vec(sentences, size=100, window=5, min_count=5, workers=4)\n",
    "\n",
    "# Create a weight matrix for the embedding layer\n",
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in w2v_model.wv:\n",
    "        embedding_matrix[i] = w2v_model.wv[word]\n",
    "\n",
    "# Define the CNN model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f6cde976-c202-4d42-9d1e-251a6b5fb6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', '.', \"'\", 'br', '-', '/><', '\"', '/>', 'movie', 'film', '.<', '(', 'one', 'like', ')', 'good', 'time', 'even', 'would', '!', 'really', 'story', 'see', '?', 'well', 'much', 'get', 'bad', 'people', 'also', 'great', ':', 'first', 'made', 'make', '...', 'way', 'could', 'movies', '<', 'characters', 'think', 'watch', 'character', 'films', 'two', 'seen', '/', 'many', 'love']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from collections import defaultdict\n",
    "freq_dict = defaultdict(int)\n",
    "\n",
    "data = pd.read_csv('Train.csv')\n",
    "for text in data['review']:\n",
    "    text = text.lower()\n",
    "    tokens = wordpunct_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    for token in tokens:\n",
    "        freq_dict[token] += 1\n",
    "\n",
    "top = sorted(freq_dict, key=freq_dict.get, reverse=True)\n",
    "top = top[:50]\n",
    "print(top)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2a38c1c2-d25b-4a93-bea0-d4332ac9ad3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/justin/work/explore/play-data-science/social-mining\n"
     ]
    }
   ],
   "source": [
    "# requires 2.0.1.rc01 to handle ratelimit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6411d9c2-976a-4b13-bf7a-bba23477068e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from github import Github\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "import configparser\n",
    "from datetime import date, timedelta\n",
    "from timeit import default_timer as timer\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e74fd0ae-6230-4535-873d-678e0d964cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_access_token():\n",
    "    parser = configparser.ConfigParser()\n",
    "    parser.read('credential.ini')\n",
    "    section = parser['github']\n",
    "    return section['access_token']\n",
    "\n",
    "def search_repo_iteratively(client, dict_list, lang, fork, stars, start, end):\n",
    "    fork_str = \"true\" if fork else \"false\"\n",
    "    # query_str = f\"lang:{lang} stars:>={stars} fork:{fork_str} archived:false template:false created:{start}..{end}\";\n",
    "    query_str = f\"lang:{lang} stars:>={stars} fork:{fork_str} created:{start}..{end}\";\n",
    "    repositories = client.search_repositories(query_str, sort=\"stars\", order=\"desc\")\n",
    "    for repo in repositories:\n",
    "        old_dict = vars(repo)\n",
    "        dict_list.append({k:v for k,v in old_dict['_rawData'].items()})\n",
    "        \n",
    "def daterange(start_date, end_date, slice):\n",
    "    tot_days = int((end_date - start_date).days) + 1\n",
    "    periods = tot_days // slice\n",
    "    remainder = tot_days % slice\n",
    "    for n in range(periods):\n",
    "        s = start_date + timedelta(n * slice)\n",
    "        e = s + timedelta(slice - 1)\n",
    "        yield (s, e)\n",
    "    if remainder != 0:\n",
    "        s = start_date + timedelta(periods * slice)\n",
    "        e = s + timedelta(remainder - 1)\n",
    "        yield (s, e)\n",
    "\n",
    "def yearrange(start_year, end_year, slice):\n",
    "    tot = end_year - start_year + 1\n",
    "    periods = tot // slice\n",
    "    remainder = tot % slice\n",
    "    for n in range(periods):\n",
    "        s = start_year + n * slice\n",
    "        e = s + slice - 1\n",
    "        yield (date(s, 1, 1), date(e, 12, 31))\n",
    "    if remainder != 0:\n",
    "        s = start_year + periods * slice\n",
    "        e = s + remainder - 1\n",
    "        yield (date(s, 1, 1), date(e, 12, 31))\n",
    "        \n",
    "def format_date(d):\n",
    "    return d.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "def collect_data(start_year, end_year, extra_year_range, lang, fork, stars, slice, subdir):\n",
    "    sub = Path(subdir)\n",
    "    sub.mkdir(exist_ok=True)\n",
    "    client = Github(load_access_token(), per_page=100)\n",
    "    date_ranges = []\n",
    "    for t in yearrange(start_year, end_year, 2):\n",
    "        date_ranges.append(t)\n",
    "    if (extra_year_range != None):\n",
    "        date_ranges.append(extra_year_range)\n",
    "        \n",
    "    date_ranges = date_ranges[::-1]\n",
    "    for date_range in date_ranges:\n",
    "        t0 = timer()\n",
    "        dict_list = []\n",
    "        for t in daterange(date_range[0], date_range[1], slice):\n",
    "            search_repo_iteratively(client, dict_list, lang, fork, stars, format_date(t[0]), format_date(t[1]))\n",
    "        t1 = timer()\n",
    "        print(f\"Collect {lang} data between {date_range[0]} and {date_range[1]} took {t1-t0} seconds\")\n",
    "        df = pd.DataFrame(dict_list)\n",
    "        df.to_excel(\"%s/%s-repo-%d-%s-%s.xlsx\" % (subdir, lang, stars, date_range[0], date_range[1]))\n",
    "        t2 = timer()\n",
    "        print(f\"Save {lang} data between {date_range[0]} and {date_range[1]} took {t2-t1} seconds\")\n",
    "    \n",
    "    t3 = timer()\n",
    "    cost_dfs = []\n",
    "    for date_range in date_ranges:\n",
    "        df = pd.read_excel(\"%s/%s-repo-%d-%s-%s.xlsx\" % (subdir, lang, stars, date_range[0], date_range[1]))\n",
    "        cost_dfs.append(df)\n",
    "    combined = pd.concat(cost_dfs)\n",
    "    combined.to_excel(\"%s/%s-repo-%d-combined.xlsx\" % (subdir, lang, stars))\n",
    "    t4 = timer()\n",
    "    print(f\"Combine and save {lang} data took {t4-t3} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c1b16b-a4c8-47a5-aa22-566f7c053833",
   "metadata": {},
   "source": [
    "### collect golang repositories with stars>=10, including folked(compare to GHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f28c1699-b715-4338-a924-44da29ce19ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collect golang data between 2023-01-01 and 2023-08-11 took 70.19333025906235 seconds\n",
      "Save golang data between 2023-01-01 and 2023-08-11 took 2.9992968959268183 seconds\n",
      "Collect golang data between 2022-01-01 and 2022-12-31 took 176.9628741350025 seconds\n",
      "Save golang data between 2022-01-01 and 2022-12-31 took 7.681846981053241 seconds\n",
      "Collect golang data between 2020-01-01 and 2021-12-31 took 496.4912161080865 seconds\n",
      "Save golang data between 2020-01-01 and 2021-12-31 took 21.172533147968352 seconds\n",
      "Collect golang data between 2018-01-01 and 2019-12-31 took 520.5672592349583 seconds\n",
      "Save golang data between 2018-01-01 and 2019-12-31 took 21.336455759010278 seconds\n",
      "Collect golang data between 2016-01-01 and 2017-12-31 took 416.8428523009643 seconds\n",
      "Save golang data between 2016-01-01 and 2017-12-31 took 16.97026139998343 seconds\n",
      "Collect golang data between 2014-01-01 and 2015-12-31 took 283.73086229898036 seconds\n",
      "Save golang data between 2014-01-01 and 2015-12-31 took 12.62168198893778 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Request GET /search/repositories?sort=stars&order=desc&q=lang%3Agolang+stars%3A%3E%3D10+fork%3Atrue+created%3A2013-08-08..2013-08-22&per_page=100 failed with 403: Forbidden\n",
      "Setting next backoff to 2.187122s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collect golang data between 2012-01-01 and 2013-12-31 took 108.50337658403441 seconds\n",
      "Save golang data between 2012-01-01 and 2013-12-31 took 4.349049465032294 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Request GET /search/repositories?sort=stars&order=desc&q=lang%3Agolang+stars%3A%3E%3D10+fork%3Atrue+created%3A2010-07-15..2010-07-29&per_page=100 failed with 403: Forbidden\n",
      "Setting next backoff to 14.011813s\n",
      "Request GET /search/repositories?sort=stars&order=desc&q=lang%3Agolang+stars%3A%3E%3D10+fork%3Atrue+created%3A2011-10-08..2011-10-22&per_page=100 failed with 403: Forbidden\n",
      "Setting next backoff to 35.180746s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collect golang data between 2010-01-01 and 2011-12-31 took 91.34662862902042 seconds\n",
      "Save golang data between 2010-01-01 and 2011-12-31 took 0.8600214379839599 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Request GET /search/repositories?sort=stars&order=desc&q=lang%3Agolang+stars%3A%3E%3D10+fork%3Atrue+created%3A2008-12-26..2009-01-09&per_page=100 failed with 403: Forbidden\n",
      "Setting next backoff to 38.737497s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collect golang data between 2008-01-01 and 2009-12-31 took 70.71417723502964 seconds\n",
      "Save golang data between 2008-01-01 and 2009-12-31 took 0.09253387490753084 seconds\n",
      "Combine and save golang data took 155.88087240199093 seconds\n",
      "CPU times: user 4min 40s, sys: 4.44 s, total: 4min 44s\n",
      "Wall time: 41min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "subdir=\"ghs-compare\"\n",
    "fork = True\n",
    "stars = 10\n",
    "slice = 15\n",
    "lang=\"golang\"\n",
    "start_year = 2008\n",
    "end_year = 2022\n",
    "extra = (date(2023, 1, 1), date(2023, 8, 11))\n",
    "collect_data(start_year, end_year, extra, lang, fork, stars, slice, subdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9aa822-74eb-42ce-a816-206e0e408065",
   "metadata": {},
   "source": [
    "### combine all golang repositories with stars>=10, non-fork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f55de9de-0a1b-4ecf-afa5-7d1154aae2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_dfs = []\n",
    "for date_range in date_ranges:\n",
    "    df = pd.read_excel(\"round2/%s-repo-%d-%s-%s.xlsx\" % (lang, stars, date_range[0], date_range[1]))\n",
    "    cost_dfs.append(df)\n",
    "combined = pd.concat(cost_dfs)\n",
    "combined.to_csv('round2/combined.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dc9e74-4415-40ce-b022-d523bf748565",
   "metadata": {},
   "source": [
    "## DEBUGGING CELLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffcd3dd-7a9d-4a7b-a497-41a2fb562e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = date(2023, 6, 1)\n",
    "end_date = date(2023, 7, 31)\n",
    "\n",
    "for t in daterange(start_date, end_date, 15):\n",
    "    print(t[0], t[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a12c78-16dc-4089-8922-8337c16a3ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Github(load_access_token(), per_page=100)\n",
    "user = client.get_user(\"schnell18\")\n",
    "repo = user.get_repo(\"influx-demo\")\n",
    "for gazer in repo.get_stargazers():\n",
    "    print(gazer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "49b8c4d7-f8e0-486d-931c-ffd502abb73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2008-01-01 2010-12-31\n",
      "2011-01-01 2013-12-31\n",
      "2014-01-01 2016-12-31\n",
      "2017-01-01 2019-12-31\n",
      "2020-01-01 2022-12-31\n"
     ]
    }
   ],
   "source": [
    "start_year = 2008\n",
    "end_year = 2022\n",
    "\n",
    "for t in yearrange(start_year, end_year, 3):\n",
    "    print(t[0], t[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

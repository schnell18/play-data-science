{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d224ea8b-1b6f-4118-8198-fedc8b9c2eaa",
   "metadata": {},
   "source": [
    "## Explore how to merege similar labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce42fb7a-6f80-458e-91ff-071895b691ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "899b0776-4e50-45b4-ae54-206dc911b989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /Users/justin/python-envs/social-mining/lib/python3.11/site-packages/pygobuildinfo-0.2.0-py3.11-macosx-13-x86_64.egg is deprecated. pip 23.3 will enforce this behaviour change. A possible replacement is to use pip for package installation..\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /Users/justin/python-envs/social-mining/lib/python3.11/site-packages/pygraphviz-1.11-py3.11-macosx-13-x86_64.egg is deprecated. pip 23.3 will enforce this behaviour change. A possible replacement is to use pip for package installation..\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /Users/justin/python-envs/social-mining/lib/python3.11/site-packages/PyGithub-2.0.1rc0-py3.11.egg is deprecated. pip 23.3 will enforce this behaviour change. A possible replacement is to use pip for package installation..\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting ntlk\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/3f/dc/5527404ed49b91a7871839f171bfdc8cf27af54cb7264a3a184581f5c726/ntlk-1.0.3.tar.gz (8.0 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting sklearn\n",
      "  Using cached sklearn-0.0.post7-py3-none-any.whl\n",
      "Building wheels for collected packages: ntlk\n",
      "  Building wheel for ntlk (pyproject.toml) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for ntlk \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[13 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m installing to build/bdist.macosx-13-x86_64/wheel\n",
      "  \u001b[31m   \u001b[0m running install\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m You probably meant to install NLTK (the Natural Language Toolkit).\n",
      "  \u001b[31m   \u001b[0m You have attempted to install NTLK instead. This is an empty package\n",
      "  \u001b[31m   \u001b[0m to help prevent typosquatting. To install NLTK, try this instead:\n",
      "  \u001b[31m   \u001b[0m pip install nltk\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m Stay safe,\n",
      "  \u001b[31m   \u001b[0m tweedge <3\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for ntlk\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25hFailed to build ntlk\n",
      "\u001b[31mERROR: Could not build wheels for ntlk, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install ntlk sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2061df1b-2163-4ee7-913e-ada4f14529ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def text_similarity(text1, text2):\n",
    "    # Tokenize and lemmatize the texts\n",
    "    tokens1 = word_tokenize(text1)\n",
    "    tokens2 = word_tokenize(text2)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens1 = [lemmatizer.lemmatize(token) for token in tokens1]\n",
    "    tokens2 = [lemmatizer.lemmatize(token) for token in tokens2]\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = stopwords.words('english')\n",
    "    tokens1 = [token for token in tokens1 if token not in stop_words]\n",
    "    tokens2 = [token for token in tokens2 if token not in stop_words]\n",
    "\n",
    "    # Create the TF-IDF vectors\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vector1 = vectorizer.fit_transform(tokens1)\n",
    "    vector2 = vectorizer.transform(tokens2)\n",
    "\n",
    "    # Calculate the cosine similarity\n",
    "    similarity = cosine_similarity(vector1, vector2)\n",
    "\n",
    "    return similarity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

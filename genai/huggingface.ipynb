{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e76100-9e47-435b-82d8-9ee6f2e023de",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c610ec55-601c-41e5-a4eb-997eced9a1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6349f875-1cd4-4941-b033-ab9848728063",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\n",
    "    \"This is my first stab at AutoTokenizer\",\n",
    "    \"Life is short, keep calm and learn AI\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b2306ee-4f10-4003-8b78-8f88bdc66020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [1986, 374, 847, 1156, 26964, 518, 8979, 37434, 25749, 374, 2805, 11, 2506, 19300, 323, 3960, 15235], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      " face rotherrenthibitueFile Brigham wool r March, everyone constitutesay cryPath\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer3 = AutoTokenizer.from_pretrained('Qwen/Qwen-7B', trust_remote_code=True)\n",
    "encoding = tokenizer3(*inputs)\n",
    "print(encoding)\n",
    "\n",
    "print(tokenizer.decode(encoding[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b9ee7e86-1d3f-4031-8551-9cc5516e5cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [1212, 318, 616, 717, 8303, 379, 11160, 30642, 7509, 14662, 318, 1790, 11, 1394, 9480, 290, 2193, 9552], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "This is my first stab at AutoTokenizerLife is short, keep calm and learn AI\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "encoding = tokenizer(*inputs)\n",
    "print(encoding)\n",
    "\n",
    "print(tokenizer.decode(encoding[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7699850-293d-44b1-a339-0c0e6a9a7ebd",
   "metadata": {},
   "source": [
    "### Baichuan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3fe6a745-7cae-42ba-920c-c599b1bb128f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'MODEL_FOR_CAUSAL_IMAGE_MODELING_MAPPING' from 'transformers.models.auto' (/Users/justin/python-envs/social-mining/lib/python3.11/site-packages/transformers/models/auto/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneration\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GenerationConfig\n\u001b[1;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbaichuan-inc/Baichuan2-13B-Chat\u001b[39m\u001b[38;5;124m\"\u001b[39m, use_fast\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbaichuan-inc/Baichuan2-13B-Chat\u001b[39m\u001b[38;5;124m\"\u001b[39m, device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/python-envs/social-mining/lib/python3.11/site-packages/transformers/generation/utils.py:29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeepspeed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_deepspeed_zero3_enabled\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_outputs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CausalLMOutputWithPast, Seq2SeqLMOutput\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     30\u001b[0m     MODEL_FOR_CAUSAL_IMAGE_MODELING_MAPPING,\n\u001b[1;32m     31\u001b[0m     MODEL_FOR_CAUSAL_LM_MAPPING,\n\u001b[1;32m     32\u001b[0m     MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING,\n\u001b[1;32m     33\u001b[0m     MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING,\n\u001b[1;32m     34\u001b[0m     MODEL_FOR_VISION_2_SEQ_MAPPING,\n\u001b[1;32m     35\u001b[0m )\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExplicitEnum, ModelOutput, is_accelerate_available, logging\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbeam_constraints\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DisjunctiveConstraint, PhrasalConstraint\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'MODEL_FOR_CAUSAL_IMAGE_MODELING_MAPPING' from 'transformers.models.auto' (/Users/justin/python-envs/social-mining/lib/python3.11/site-packages/transformers/models/auto/__init__.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.generation.utils import GenerationConfig\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"baichuan-inc/Baichuan2-13B-Chat\", use_fast=False, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"baichuan-inc/Baichuan2-13B-Chat\", device_map=\"auto\", torch_dtype=torch.bfloat16, trust_remote_code=True)\n",
    "model.generation_config = GenerationConfig.from_pretrained(\"baichuan-inc/Baichuan2-13B-Chat\")\n",
    "messages = []\n",
    "messages.append({\"role\": \"user\", \"content\": \"解释一下“温故而知新”\"})\n",
    "response = model.chat(tokenizer, messages)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8859bc6-d3fa-4958-b519-da40ec8550aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
